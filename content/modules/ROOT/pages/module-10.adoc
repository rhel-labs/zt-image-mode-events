= Repurposing a bootc host

In addition to simplifying updates and providing native rollback, operating RHEL in image mode also makes it simple to quickly change the purpose of a running system.

The `switch` operation allows adminstrators to change the system software to a different image, while keeping local system configuration and data intact.

In this lab we'll explore this feature as well as expand on the idea of standardized builds and derived images.

[#write-containerfiles]
== Testing the developers bug report

While testing their proof of concept, the developers found that the image they created worked fine when a machine was newly installed.
To help save time, they wanted to repurpose a few other image mode machines instead of requesting brand new infrastructure.
When they switched the running machine, the application wouldn't start.
They only had a RHEL 9.6 host to test, and they weren't sure if moving to RHEL 10.1 was part of the problem.

Change to the application git repository with the bootc Containerfile, cloned onto the build host.
[source,bash,role="execute",subs=attributes+]
----
cd ~/bootc-version
----

First, let's add linting and then build a fresh copy to test the switch on our own existing image mode host.
[source,bash,role="execute",subs=attributes+]
----
nano Containerfile
----

Add the following line as the last instruction in the Containerfile.
We always want linting to be the final step in a build.
[source,dockerfile,role="execute",subs=attributes+]
----
RUN bootc container lint
----

The linter is designed to issue warnings about potential problems and will error and stop the build if there is something that would stop an image mode host from operating. 
Some warnings are harmless, but others could point to issues with certain types of operations during the lifetime of a host.

== Build and push the image

When we build this image, we will use a new name to denote its new purpose. Your naming and tagging conventions should aim to convey information to the people who need them as much as providing hooks to automate and control visibility to hosts.

[source,bash,role="execute",subs=attributes+]
----
podman build --file Containerfile --tag registry-{guid}.{domain}/app-test
----
no errors, but we do see warnings about content in `/var` about logs, directories, and other files. 
....
STEP 15/15: RUN bootc container lint
Lint warning: var-log: Found non-empty logfiles:
  /var/log/dnf.librepo.log
  /var/log/dnf.log
  /var/log/dnf.rpm.log
  /var/log/hawkey.log
  /var/log/rhsm/rhsm.log

Lint warning: var-tmpfiles: Found content in /var missing systemd tmpfiles.d entries:
  d /var/lib/dnf 0755 root root - -
  d /var/lib/nginx 0770 nginx root - -
  d /var/lib/nginx/tmp 0770 nginx root - -
  d /var/log/nginx 0711 root root - -
  d /var/roothome/.cache 0755 root root - -
  ...and 128 more
Found non-directory/non-symlink files in /var:
  var/lib/dnf/history.sqlite
  var/lib/dnf/history.sqlite-shm
  var/lib/dnf/history.sqlite-wal
  var/lib/rhsm/cache/productid_repo_mapping.json
  var/lib/rhsm/productid.js
  ...and 66 more
....

There's quite a few warnings, so our culprit may be in there. 
[#build]

Push this image to the registry so we can use it to test on our existing target host.
[source,bash,role="execute",subs=attributes+]
----
podman push registry-{guid}.{domain}/app-test
----


[#switch-run]
== Switch and test the image

_Change to the tab labeled "Ops SSH session"_

You saw earlier how `bootc` tracks the image in the registry to find updates.
To change to a different image, use the `bootc switch` command to provide a different image specification.
We'll use it to change into the testing image we just pushed to the registry.
[source,bash,role="execute",subs=attributes+]
----
sudo bootc switch registry-{guid}.{domain}/app-test
----

From an output perspective, a switch looks the same as an update. There's a new deployment that's been staged and prepared for the next boot, but the image reference is completely different.

[source,bash,role="execute",subs=attributes+]
----
sudo bootc status
----
....
  Staged image: registry-89nsj.apps.ocpvdev01.rhdp.net/app-test
        Digest: sha256:b2c3d8c7327d13087324c5a60f70b79e9b3e415cba32920895c76d688c9c8629 (amd64)
       Version: 10.1 (2025-12-11T06:11:35Z)

● Booted image: registry-89nsj.apps.ocpvdev01.rhdp.net/base
        Digest: sha256:ec5f5f46fd530ea8b4e393b276ab3d09dfce9012df8999073321369cc4659b8e (amd64)
       Version: 10.1 (2025-12-11T06:11:35Z)

  Rollback image: registry-89nsj.apps.ocpvdev01.rhdp.net/base
          Digest: sha256:8e1eaea2f90fcdce6853392b47721037857fd6310b8f801ac3527b14df3c7c8d (amd64)
         Version: 10.1 (2025-12-11T06:11:35Z)
....

Issue a full reboot to finalize the switch, making sure you're on the right host.

[source,bash,role="execute",subs=attributes+]
----
sudo systemctl reboot
----

[#layers]
== Troubleshooting derived builds
===== _Log back into the VM by clicking the `reconnect` button or the refresh icon in the tab title._

You should see the failed nginx unit on login, so check the status output of the service to see what happened.
....
[systemd]
Failed Units: 1
  nginx.services
....
[source,bash,role="execute",subs=attributes+]
----
systemctl status nginx.service --no-pager
----

....
nginx[1078]: nginx: [alert] could not open error log file: open() "/var/log/nginx/error.log…irectory)
nginx[1078]: 2025/11/17 16:18:10 [emerg] 1078#1078: mkdir() "/var/lib/nginx/tmp/client_body…irectory)
nginx[1078]: nginx: configuration file /etc/nginx/nginx.conf test failed
....

Looks the culprit is nginx missing its `/var` file structure but why does that not happen when the developers provision a new machine?

==== Local machine state vs central image state
In an image mode system, `bootc` handles the following directories differently, which is what allows for the seamless update and rollback experience. 

  * `/usr` -> image state, contents of the image will be extracted and overwrite local files
  * `/etc` -> local configuration state, contents of the image are merged with a preference for local files
  * `/var` -> local state, contents of the image will be ignored after the initial installation of any image

Initial nginx logs and workfing files created during the build in `/var` are created as part of an *install* deployment.
During a `switch` or `upgrade`, this is now *local state* meaning `bootc` won't deploy those changes to a running system.
So the directories created by the RPM aren't created on the running host, which explains why a fresh install launches the app but a switch fails. 
This is one of the new mental models that needs to be adopted to handle image mode operations. 
This is also why user home directories are symlinked from the typical `/home` to `/var/home`, ensuring that new updates aren't overwriting user data.
Generally, anything that needs to be permanent on the host should live in `/var` and options like symlinks are available to account for existing application behavior.

If `/var` is hands-off after initial provisioning, how can you account for common directories that need to be present in `/var` on all systems?
By using `systemd tmpfiles` to manage them at boot.

We don't expect you to handle all of these cases manually. 
Upstream packages have been moving to use `systemd tmpfiles` and `systemd sysusers` to handle user and file creation for some time.
The introduction of image mode has put some pressure on this migration, but you can already see the `nginx` package https://src.fedoraproject.org/rpms/nginx/blob/aff374fc9038f31a53370f5779f4d03df8fbbc6d/f/nginx.tmpfiles[already has a fix upstream.,window=_blank]

== Workaround with tmpfiles.d
===== _Switch to the tab labeled "Build host terminal"_

Let's use the upstream fix to create a local version of nginx.tmpfiles in the system location. 
This means when the new RPM lands, we can just remove this from our Containerfile and the system will continue to work as expected.

Let's add a `heredoc` just *before* the linter in the Containerfile to create the new file for testing.
[source,bash,role="execute",subs=attributes+]
----
nano Containerfile
----
[source,dockerfile,role="execute",subs=attributes+]
----
RUN <<EORUN
    set -exuo pipefail
    echo "d /var/lib/nginx     770 nginx root -" >> /usr/lib/tmpfiles.d/nginx.conf
    echo "d /var/lib/nginx/tmp 770 nginx root -" >> /usr/lib/tmpfiles.d/nginx.conf
    echo "d /var/log/nginx     711 root  root -" >> /usr/lib/tmpfiles.d/nginx.conf
EORUN
----

Using the `heredoc` let's us run multiple commands as a single layer and can be very handy.
Since this is executed during a build like any other shell `heredoc`, we need to use some `set` magic to make sure a failure here stops the build.

We can also clean up a few of those other errors by removing the workfing caches from `/var` for things like `dnf` since these won't be needed after build.

[source,dockerfile,role="execute",subs=attributes+]
----
RUN rm /var/{cache,lib}/dnf /var/lib/rhsm /var/cache/ldconfig -rf
----

Save the file and move on.

To test that `systemd` will handle the directory creation on a switch and let the app now start, we'll build this image with a new tag.

[source,bash,role="execute",subs=attributes+]
----
podman build --file Containerfile --tag registry-{guid}.{domain}/app-test
----
Note the linter isn't complaining about `nginx` in `/var` any more.
....
Lint warning: var-tmpfiles: Found content in /var missing systemd tmpfiles.d entries:
  d /var/lib/dnf 0755 root root - -
  d /var/roothome/.cache 0755 root root - -
  d /var/roothome/.cache/pip 0755 root root - -
  d /var/roothome/.cache/pip/http-v2 0755 root root - -
  d /var/roothome/.cache/pip/http-v2/0 0755 root root - -
....
And of course push it to the local registry:

[source,bash,role="execute",subs=attributes+]
----
podman push registry-{guid}.{domain}/app-test
----

=== Test the Workaround
_Switch to the tab labeled "Ops SSH session"_

If we were to try to use switch again, `bootc` would let us know that the image was the same so there's nothing to switch over.
Instead we'd need to use `update` to pull in any changes to the same image.

[source,bash,role="execute",subs=attributes+]
----
sudo bootc switch registry-{guid}.{domain}/app-test
----
....
Image specification is unchanged.
....
How can we test the problem with switching and `/var`?

=== Native roll back capabilities.
However, there's a way we can turn back time with image mode hosts: `bootc rollback`.
This option will swap the currently booted deployment with the most recent one, kept on disk.
You will see this deployment listed in `bootc status`.


[source,bash,role="execute",subs=attributes+]
----
sudo bootc status
----
....
● Booted image: registry-mjghb.apps.ocpvdev01.rhdp.net/app-test
        Digest: sha256:10eb5e2ebba0e549deb43315786731ff976d544f6f682c4cf30de0f4c371a059 (amd64)
       Version: 10.1 (2025-11-13T17:17:56Z)

  Rollback image: registry-mjghb.apps.ocpvdev01.rhdp.net/base
          Digest: sha256:d83d9f48a16e98c2d8ca2240f99ce0e2d2ffc5bc948d0434bff1810d2b58e6d5 (amd64)
         Version: 10.1 (2025-11-13T17:30:16Z)
....

Rolling back a `bootc` deployment just tells the bootloader to boot the previous deployment.
Reversing the logic from our filesystem discussion, swapping the deployment for an earlier one on disk means that `/etc` and `/usr` will revert to that point in time but not `/var`.
This will put the system in the same state as we started the exercise.

Use the `--apply` flag along with `rollback` to immediately switch back to the previous deployment.
[source,bash,role="execute",subs=attributes+]
----
sudo bootc rollback --apply
----

After a short while, you can log back in to the virtual machine by clicking `reconnect` or using the refresh button in the tab.

Looking at `bootc status`, we can see the booted image is now the `base` image and the app-test image is now the new rollback. 
[source,bash,role="execute",subs=attributes+]
----
sudo bootc status
----
....
● Booted image: registry-mjghb.apps.ocpvdev01.rhdp.net/base
        Digest: sha256:10eb5e2ebba0e549deb43315786731ff976d544f6f682c4cf30de0f4c371a059 (amd64)
       Version: 10.1 (2025-11-13T17:17:56Z)

  Rollback image: registry-mjghb.apps.ocpvdev01.rhdp.net/app-test
          Digest: sha256:d83d9f48a16e98c2d8ca2240f99ce0e2d2ffc5bc948d0434bff1810d2b58e6d5 (amd64)
         Version: 10.1 (2025-11-13T17:30:16Z)
....

We can now re-run the `switch` to test the tmpfiles creation.
Let's use the `--apply` flag here as well and immediately execute the reboot instead of checking the status first.
[source,bash,role="execute",subs=attributes+]
----
sudo bootc switch registry-{guid}.{domain}/app-test --apply
----

After a short while, you can log back in to the virtual machine by clicking `reconnect` or using the refresh button in the tab.

There's no systemd warning on login, so check the status of nginx and the application.
[source,bash,role="execute",subs=attributes+]
----
systemctl status nginx.service --no-pager
----
....
Nov 17 17:03:33 imrhel systemd[1]: Starting nginx.service - The nginx HTTP and reverse proxy server...
Nov 17 17:03:34 imrhel nginx[1055]: nginx: the configuration file /etc/nginx/nginx.conf syntax is ok
Nov 17 17:03:34 imrhel nginx[1055]: nginx: configuration file /etc/nginx/nginx.conf test is successful
Nov 17 17:03:34 imrhel systemd[1]: Started nginx.service - The nginx HTTP and reverse proxy server.
....

[source,bash,role="execute",subs=attributes+]
----
systemctl status info-app.service --no-pager
----
As a final sanity check, is the app http://hostinfo-{guid}.{domain}/[up and running?,window=_blank]

The host details page is up and running on the test host. With that solved, we can start on migrating the app from the Red Hat base to our corporate baseline.


== Core principles
Recovering from issues with installed software without resorting to an expensive recovery process is a key benefit of the image mode operating model.
Changing the deployed system software with a single command to easily reuse capacity makes testing faster and cheaper than the typical need to deploy a new stack.
Knowing how changes to the filesystem are managed by `bootc` is important to navigating the differences between image mode and package mode.

