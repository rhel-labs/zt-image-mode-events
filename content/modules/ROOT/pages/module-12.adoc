= Using container practices with our OS builds

In addition to simplifying updates and providing native rollback, operating RHEL in image mode also makes it simple to adopt application container development practices for operating system definiteions. 

In the last lab, we imposed some structure on application container.
In this lab, we'll take a pattern from the application container world to move the SELinux module creation.

[#write-containerfiles]
== Using multi-stage builds
==== _Click on the tab labeled "Build host"_ 

One common pattern in container builds is offloading the creation of binary components that need to be generated at build time, but don't need to carry all of the dependencies into the final image. 
The OCI specification allows for multiple image and assoociated actions to be defined in a single Containerfile.
This is refered to as a multi-stage build and can be used to separate portions of the build while making still accessible later in the process. 

Let's explore how that can work by looking at the SELinux module creation in the Containerfile.

Here's the block in the Containerfile that builds and implements the module:
[source,dockerfile,role="execute",subs=attributes+]
----
# Ensure nginx can talk to gunicorn
WORKDIR /app # <.>
RUN checkmodule -M -m nginx_connect_flask_sock.te -o nginx_connect_flask_sock.mo # <.>
RUN semodule_package -o nginx_connect_flask_sock.pp -m nginx_connect_flask_sock.mo
RUN semodule -i nginx_connect_flask_sock.pp # <.>
RUN mkdir /run/flask-app && chgrp -R nginx /run/flask-app && chmod 770 /run/flask-app # <.>
RUN semanage fcontext -a -t httpd_var_run_t /run/flask-app
----
<.> Doing work inside the production application target directory in the image
<.> Intermediate files created in the image
<.> The final binary policy we really need, but in the application directory
<.> Other work needed to make things take effect

The `WORKDIR` directive is what's creating the intermedite build files inside the image.
The `WORKDIR` target (`/app`) is essentially a `cd` inside the *image filesystem*, not the build host.

Let's rearrange the work for this module to use a multi-stage build and not carry around extra files in the application directory.
[source,bash,role="execute",subs=attributes+]
----
nano Containerfile
----

Each stage is defined by a new `FROM` line and can use any image needed.
What makes them particularly useful is the ability to selectively copy from one stage into another, leaving behind everything else.

At the top of the Containerfile, add the following block that has the build lines for the SELinux module:
[source,dockerfile,role="execute",subs=attributes+]
----
FROM registry.redhat.io/rhel10/rhel-bootc:10.1 AS policy # <.>
COPY app/nginx_connect_flask_sock.te . <.>
RUN checkmodule -M -m nginx_connect_flask_sock.te -o nginx_connect_flask_sock.mo
RUN semodule_package -o nginx_connect_flask_sock.pp -m nginx_connect_flask_sock.mo
----
<.> Using `AS` let's us access the contents of the stage later via that name
<.> We copy from the host `app` directory instead of a directory the image

We don't need to change the actual work, just where it's performed.
We could have used any RHEL 10 base image, like UBI, since there's nothing specific to the image mode base for the SELinux tools.

Second, change the secure baseline `FROM` line to include a name.
This is mostly for readability since we don't have a third stage
[source,dockerfile,role="execute",subs=attributes+]
----
FROM registry-{guid}.{domain}/base AS host <.>
----
<.> name our application definition stage

Finally, replace the old block with a copy from the `policy` stage
[source,dockerfile,role="execute",subs=attributes+]
----
# Deploy the binary SELinux policy and update the labeling
COPY --from=policy nginx_connect_flask_sock.pp \ # <.>
/usr/share/selinux/packages/targeted/ # <.>
RUN semodule -i /usr/share/selinux/packages/targeted/nginx_connect_flask_sock.pp
RUN semanage fcontext -a -t httpd_var_run_t /run/flask-app
----
<.> `--from` tells podman to look for a matching stage for a file and copy from there instead of the host filesystem
<.> target a system location for our custom policy instead of the application directory

We can also add the policy to a standard system location managed by the image. 
This will get picked up on boot as part of the targeted policy from here on.
Since it's versioned with the image, if we need to make changes, we can make those the same way we make any other changes to the image.

There's other work in that original block, making sure the `/run` directory gets created and labeled correctly.
That shouldn't be needed as long as the label is loaded in the targeted policy and the service file is correct.

See how the new multi-stage build operates
[source,bash,role="execute",subs=attributes+]
----
podman build --file Containerfile --tag registry-{guid}.{domain}/app-test:v2
----

First thing is there's an addition to the STEP output, `[X/Y]` at the start of the line to indicate what stage the step is assoociated with.
This helps troubleshooting builds.
....
[1/2] STEP 1/4: FROM registry.redhat.io/rhel10/rhel-bootc:10.1 AS policy
[1/2] STEP 2/4: COPY app/nginx_connect_flask_sock.te .
--> d6976d459410
[1/2] STEP 3/4: RUN checkmodule -M -m nginx_connect_flask_sock.te -o nginx_connect_flask_sock.mo
--> da9bac8dd1e5
[1/2] STEP 4/4: RUN semodule_package -o nginx_connect_flask_sock.pp -m nginx_connect_flask_sock.mo
--> 3ca8e0442c1a
[2/2] STEP 1/11: FROM registry-j475j.apps.ocpvdev01.rhdp.net/base AS host
Trying to pull registry-j475j.apps.ocpvdev01.rhdp.net/base:latest...
....
[source,bash,role="execute",subs=attributes+]
----
podman push registry-{guid}.{domain}/app-test:v2
----

==== _Switch to the "Security SSH session" tab to test the latest build of the image._
[source,bash,role="execute",subs=attributes+]
----
sudo bootc update --soft-reboot=required --apply
----

As a final sanity check, is the app http://hostinfo-{guid}.{domain}/[up and running?,window=_blank]
